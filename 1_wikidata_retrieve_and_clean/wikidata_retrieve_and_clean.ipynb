{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition file for [Peña and Choi (2021, *Economics Letters*)](https://doi.org/10.1016/j.econlet.2021.109968)\n",
    "- Corresponding author for this file: Jun Ho Choi (junhoc@uchicago.edu)\n",
    "\n",
    "This `.ipynb` file contains Python code to acquire data that has been used for the article **\"Female representation among notable people born in 1700-2000\" (Peña and Choi, 2021, *Economics Letters*)**. Note that, due to the changing nature of the Wikipedia and Wikidata databases, the results of running the below process are not likely to be completely the same as the data that we collected in November 2020, which were used for our article.\n",
    "\n",
    "## 0. Importing necessary modules and packages\n",
    "\n",
    "We assume that all of the Python modules and packages mentioned in this file are installed when running the below codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.distributed as dd\n",
    "import requests as req\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from json.decoder import JSONDecodeError\n",
    "from requests.exceptions import ChunkedEncodingError\n",
    "from qwikidata.sparql import return_sparql_query_results as sparql_res\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from dask.diagnostics import ProgressBar as daskPBar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also set up the `dask` cluster here for parallelization. Note that `N_WORKERS` can be set to any other number depending on the machine's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORKERS = 4\n",
    "cluster = dd.LocalCluster(\n",
    "    n_workers=N_WORKERS, threads_per_worker=1, memory_limit=(2.5 * 1024 ** 3)\n",
    ")\n",
    "client = dd.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Query block (in SPARQL) and the initial fetch of relevant Wikidata information\n",
    "\n",
    "The below codes are necessary parts for the initial fetching of relevant Wikidata information. In the last sub-section (1.5), we will demonstrate how the codes come together to acquire relevant information.\n",
    "\n",
    "### 1.1. Query block\n",
    "\n",
    "The query block below is written in SparkQL and is the standard for fetching data from Wikidata. It will be used in conjunction with the `qwikidata` module to fetch query results to Python environment. For a detailed introduction to using SparkQL to fetch relevant information from Wikidata, please refer to [this video by the Wikimedia Foundation](https://www.youtube.com/watch?v=kJph4q0Im98)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## structure of the query\n",
    "query_block = \"\"\"\n",
    "SELECT ?person ?personLabel ?occupation ?occupationLabel ?birthplace ?birthplaceLabel ?dob ?dobLabel ?region ?regionLabel ?gregion ?gregionLabel ?ggregion ?ggregionLabel ?sex ?sexLabel\n",
    "WHERE {\n",
    "  ?person wdt:P31 wd:Q5 .\n",
    "  ?person wdt:P19 ?birthplace .\n",
    "  OPTIONAL {\n",
    "    ?birthplace wdt:P131 ?region.\n",
    "    OPTIONAL {\n",
    "      ?region wdt:P131 ?gregion.\n",
    "      OPTIONAL {\n",
    "        ?gregion wdt:P131 ?ggregion.\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  OPTIONAL {\n",
    "    ?person wdt:P106 ?occupation \n",
    "  }\n",
    "  ?birthplace wdt:P17 wd:%s .\n",
    "  \n",
    "  ?person wdt:P21 ?sex .\n",
    "  \n",
    "  ?person wdt:P569 ?dob .\n",
    "  FILTER (YEAR(?dob) = %s).\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dictionary (or `pandas.Series`) of country name to country code\n",
    "\n",
    "For each country, there exists a corresponding Wikidata-specific country code (which can be found by searching on Wikidata). We will need to map country name to country code to retrieve country-specific information from Wikidata. We may also use the `pandas.Series` format over the dictionary format. We also note that these countries are not exhaustive of the countries in Wikidata, but simply contain the 30 countries we use in our study (roughly accounting for 75 percent of the world population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note that there are multiple names for some of the countries\n",
    "wiki_dict = {\n",
    "    \"United States of America\": \"Q30\",\n",
    "    \"United States\": \"Q30\",\n",
    "    \"USA\": \"Q30\",\n",
    "    \"Germany\": \"Q183\",\n",
    "    \"France\": \"Q142\",\n",
    "    \"United Kingdom\": \"Q145\",\n",
    "    \"UK\": \"Q145\",\n",
    "    \"Italy\": \"Q38\",\n",
    "    \"Russia\": \"Q159\",\n",
    "    \"Russian Federation\": \"Q159\",\n",
    "    \"Japan\": \"Q17\",\n",
    "    \"Spain\": \"Q29\",\n",
    "    \"Brazil\": \"Q155\",\n",
    "    \"India\": \"Q668\",\n",
    "    \"Turkey\": \"Q43\",\n",
    "    \"China\": \"Q148\",\n",
    "    \"Mexico\": \"Q96\",\n",
    "    \"South Korea\": \"Q884\",\n",
    "    \"Iran\": \"Q794\",\n",
    "    \"Indonesia\": \"Q252\",\n",
    "    \"South Africa\": \"Q258\",\n",
    "    \"Colombia\": \"Q739\",\n",
    "    \"Philippines\": \"Q928\",\n",
    "    \"Pakistan\": \"Q843\",\n",
    "    \"Nigeria\": \"Q1033\",\n",
    "    \"Egypt\": \"Q79\",\n",
    "    \"Thailand\": \"Q869\",\n",
    "    \"Vietnam\": \"Q881\",\n",
    "    \"Viet Nam\": \"Q881\",\n",
    "    \"Bangladesh\": \"Q902\",\n",
    "    \"Kenya\": \"Q114\",\n",
    "    \"Democratic Republic of the Congo\": \"Q974\",\n",
    "    \"DR Congo\": \"Q974\",\n",
    "    \"Myanmar\": \"Q836\",\n",
    "    \"Ethiopia\": \"Q115\",\n",
    "    \"Tanzania\": \"Q924\",\n",
    "}\n",
    "\n",
    "wiki_name_series = pd.Series(wiki_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Functions for fetching relevant data from Wikidata\n",
    "\n",
    "#### 1.3.1. Data retrieval for a single country-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querydata_organizer(qdat, val=\"value\"):\n",
    "    \"\"\"\n",
    "    Organizing the raw JSON or dictionary version of query data, then\n",
    "    putting it into a list which can be used to create dataframes\n",
    "\n",
    "    Inputs:\n",
    "    - qdat (dict): returned query data (after making the SparQL query)\n",
    "    - val (str): the string designation for finding the necessary value\n",
    "\n",
    "    Output:\n",
    "    - result_df (list of lists): information from query, in an organized\n",
    "        fashion (easily transform-able to pandas dataframe or csv)\n",
    "    \"\"\"\n",
    "    res = qdat[\"results\"][\"bindings\"]\n",
    "\n",
    "    result_lst = []\n",
    "    for i in res:\n",
    "        ## name and wikidata URL\n",
    "        name = i[\"personLabel\"][val]\n",
    "        wikidata_url = i[\"person\"][val]\n",
    "\n",
    "        ## birthplace and where the said location(s) belong to\n",
    "        bplace = i[\"birthplace\"][val]\n",
    "        bplaceLabel = i[\"birthplaceLabel\"][val]\n",
    "        if i.get(\"regionLabel\") is None:\n",
    "            region, regionLabel = np.nan, np.nan\n",
    "        else:\n",
    "            regionLabel = i[\"regionLabel\"][val]\n",
    "            region = i[\"region\"][val]\n",
    "\n",
    "        if i.get(\"gregion\") is None:\n",
    "            gregion, gregionLabel = np.nan, np.nan\n",
    "        else:\n",
    "            gregionLabel = i[\"gregionLabel\"][val]\n",
    "            gregion = i[\"gregion\"][val]\n",
    "\n",
    "        if i.get(\"ggregion\") is None:\n",
    "            ggregion, ggregionLabel = np.nan, np.nan\n",
    "        else:\n",
    "            ggregionLabel = i[\"ggregionLabel\"][val]\n",
    "            ggregion = i[\"ggregion\"][val]\n",
    "\n",
    "        ## cleaning occupation\n",
    "        if i.get(\"occupation\") is None:\n",
    "            occupation, occupationLabel = np.nan, np.nan\n",
    "        else:\n",
    "            occupationLabel = i[\"occupationLabel\"][val]\n",
    "            occupation = i[\"occupation\"][val]\n",
    "\n",
    "        dob = i[\"dobLabel\"][val]\n",
    "        yr = dob[0:4]\n",
    "        month = dob[5:7]\n",
    "        day = dob[8:10]\n",
    "        sex = i[\"sexLabel\"][val]\n",
    "        result_lst.append(\n",
    "            [\n",
    "                name,\n",
    "                wikidata_url,\n",
    "                bplaceLabel,\n",
    "                bplace,\n",
    "                regionLabel,\n",
    "                region,\n",
    "                gregionLabel,\n",
    "                gregion,\n",
    "                ggregionLabel,\n",
    "                ggregion,\n",
    "                occupation,\n",
    "                occupationLabel,\n",
    "                yr,\n",
    "                month,\n",
    "                day,\n",
    "                sex,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return result_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_organize_pipeline(\n",
    "    year, country, qblock=query_block, wiki_name_to_code=wiki_name_series, val=\"value\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Specify a specific year (of birth), make the query, and organize the returned\n",
    "    information into a list of lists\n",
    "\n",
    "    inputs:\n",
    "    - year (int / str): year of birth for the people we are interested in\n",
    "    - country (str): country that we want to search\n",
    "    - qblock (str): query block to be used in fetching the data; use default\n",
    "        unless there is a need to fetch other data\n",
    "    - wiki_name_to_code (dict or pandas.Series): dictionary or pandas.Series that\n",
    "        maps country name to relevant country code used in Wikidata\n",
    "    - val (str): name of the value that we one to acquire; default \"value\"\n",
    "\n",
    "    outputs:\n",
    "    - if successful, then organized values from the query in list of lists\n",
    "        will be returned; if not, the unsuccessful year will be returned\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    yr = str(year)\n",
    "    cntry = str(wiki_name_to_code[country])\n",
    "    query_itself = query_block % tuple([cntry, yr])\n",
    "\n",
    "    try:\n",
    "        res = sparql_res(query_itself)\n",
    "        res_organized = querydata_organizer(res, val)\n",
    "    except (JSONDecodeError, ChunkedEncodingError) as error_tuple:\n",
    "        try:\n",
    "            res = sparql_res(query_itself)\n",
    "            res_organized = querydata_organizer(res, val)\n",
    "        except (JSONDecodeError, ChunkedEncodingError) as error_tuple:\n",
    "            res_organized = year\n",
    "\n",
    "    return res_organized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Parallelization (for multiple years) and making sure that data have been retrieved properly\n",
    "\n",
    "Due to issues such as unstable connectivity to Wikidata query services or general Internet connectivity, there may be cases in which information is not retrieved properly across all years. The following process is to ensure that all desired data is acquired while parallelizing the data retrieval process across all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiyear_querydata(\n",
    "    years,\n",
    "    country,\n",
    "    qblock=query_block,\n",
    "    wiki_name_to_code=wiki_name_series,\n",
    "    val=\"value\",\n",
    "    cl=client,\n",
    "):\n",
    "    \"\"\"\n",
    "    Specify multiple years for retrieving multiple-birth-year data from Wikidata.\n",
    "\n",
    "    Inputs:\n",
    "    - years (array-like of int): containing the relevant birth years that we want\n",
    "        to acquire information about, pertaining to the people of a specified country\n",
    "    - country (str): name of the country from which to acquire information\n",
    "    - qblock (str): query block to be used in fetching the data; use default\n",
    "        unless there is a need to fetch other data\n",
    "    - wiki_name_to_code (dict or pandas.Series): dictionary or pandas.Series that\n",
    "        maps country name to relevant country code used in Wikidata\n",
    "    - val (str): name of the value that we one to acquire; default \"value\"\n",
    "    - cl (dask.distributed.client.Client): dask client for parallelization\n",
    "\n",
    "    Outputs:\n",
    "    - gather_all (list of lists): containing all retrieved information, one row per line\n",
    "        from the returned Wikidata query\n",
    "    \"\"\"\n",
    "\n",
    "    gather_fn = lambda x: query_organize_pipeline(\n",
    "        x, country, qblock, wiki_name_to_code, val\n",
    "    )\n",
    "\n",
    "    target_years = list(years)\n",
    "    incomplete = True\n",
    "    gather_completed = []\n",
    "    while incomplete:\n",
    "        cl.restart()\n",
    "        gather = cl.map(gather_fn, target_years)\n",
    "        gather = cl.gather(gather)\n",
    "\n",
    "        check_incomplete = [x for x in gather if type(x) == int]\n",
    "        gather_completed += [x for x in gather if type(x) != int]\n",
    "\n",
    "        if len(check_incomplete) == 0:\n",
    "            incomplete = False\n",
    "        else:\n",
    "            target_years = check_incomplete\n",
    "            print(\n",
    "                \"re-collecting data, has {} incomplete years.\".format(len(target_years))\n",
    "            )\n",
    "\n",
    "    gather_all = []\n",
    "    for i in gather_completed:\n",
    "        gather_all += i\n",
    "\n",
    "    return gather_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Cleaning data, while separating out the Jan. 1st-birthdays and non-Jan. 1st-birthdays\n",
    "\n",
    "One particular issue that we detected while trying to use the retrieved information from Wikidata was there were those with unclear birth years. For instance, see the case of [Ira Washington Rubel](https://www.wikidata.org/wiki/Q1650128) whose date of birth is noted as `19. century` instead of having a specific birth year. This would not pose a great problem if Wikidata query simply returns birth year as \"unknown\" or \"not available\"; however, in these cases, Wikidata query would still return some numbers for the unavailable birth years (and birth months and days as well). Again in the example with Ira Washington Rubel, the birth year, month, and day were returned as `1900`, `01`, and `01`. Similarly, those that have their date of birth information as `20. century` on the Wikidata website would have the birth year, month, and day information returned as `2000`, `01`, and `01`.\n",
    "\n",
    "This issue can be problematic because, if a researcher were to use birth year data without much precaution, then one may find a lot of those with misspecified birth years. At the same time, one may also have problem distinguishing from, say, those that were actually born in the year 1900 and those whose birth year information is simply noted as `1900` due to how the Wikidata queries are returned. One pattern that we are able to observe is that, whenever birth years are unknown exactly, their Wikidata webpage would also lack birth month and day information, and Wikidata query would return **January 1st** (or `01` and `01` as birth month and day, respectively).\n",
    "\n",
    "Acknowledging this rather minor but potentially important issue, we follow the below procedure:\n",
    "1. Knowing that those with incomplete birth date information would have January 1st (`01` and `01`) as their birth month and day information, separate out those having birth month-day as January 1st versus those not having the said birth month-day. Retain the information of the latter group of people.\n",
    "2. Reference back to the Wikidata website (as opposed to the query) for the people with January 1st as their query-specified birth month-days to see if their birth month-days are the said day. If so, retain those whose birth month-days are verified to be January 1st.\n",
    "3. For those whose Wikidata webpages indicate their birth month-days are not January 1st, retain if their birth **years** are well-specified (e.g., 1900 instead of 19. century). If not, drop their information.\n",
    "\n",
    "For this sub-section, we introduce a function to prepare for the above process by (i) separating out those born on January 1st and those not and (ii) organize information in a `pandas.DataFrame` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_j1_vs_nj1(df_of_lst):\n",
    "    \"\"\"\n",
    "    Organizing the data retrieved from Wikidata queries (which is in list format)\n",
    "    into pandas.DataFrame, and separating January 1st birthdays and non-January 1st\n",
    "    birthdays.\n",
    "\n",
    "    Inputs:\n",
    "    - df_of_lst (array-like of array-likes): resulting information from running\n",
    "        the function `multiyear_querydata`, which has retrieved information from\n",
    "        relevant Wikidata queries\n",
    "\n",
    "    Outputs:\n",
    "    - j1, nj1 (pandas DataFrames): organized Wikidata query information, where the\n",
    "        former contains those people's information with birth month-days as\n",
    "        January 1st, and the latter contains those with birth month-days not as\n",
    "        January 1st\n",
    "    \"\"\"\n",
    "\n",
    "    df_col = [\n",
    "        \"person\",\n",
    "        \"personCode\",\n",
    "        \"birthplace\",\n",
    "        \"birthplaceCode\",\n",
    "        \"birthregion\",\n",
    "        \"birthregionCode\",\n",
    "        \"birthgregion\",\n",
    "        \"birthgregionCode\",\n",
    "        \"birthggregion\",\n",
    "        \"birthggregionCode\",\n",
    "        \"occupation\",\n",
    "        \"occupationCode\",\n",
    "        \"dob_year\",\n",
    "        \"dob_month\",\n",
    "        \"dob_day\",\n",
    "        \"sex\",\n",
    "    ]\n",
    "\n",
    "    stacked = np.vstack(df_of_lst)\n",
    "    df = pd.DataFrame(stacked, columns=df_col).astype(\n",
    "        {\"dob_year\": \"int64\", \"dob_month\": \"int64\", \"dob_day\": \"int64\"}\n",
    "    )\n",
    "\n",
    "    j1 = df.loc[(df[\"dob_month\"] == 1) & (df[\"dob_day\"] == 1), :].copy()\n",
    "    nj1 = df.loc[(df[\"dob_month\"] != 1) | (df[\"dob_day\"] != 1), :].copy()\n",
    "\n",
    "    return j1, nj1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Demonstration for this section's codes\n",
    "\n",
    "Here, we demonstrate extracting the information for those who were born in France between the years 1900 to 1950 (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-collecting data, has 4 incomplete years.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "## fetching data, then dividing between January 1st cases\n",
    "## and non-January 1st cases\n",
    "EXAMPLE_J1, EXAMPLE_NJ1 = organize_j1_vs_nj1(\n",
    "    multiyear_querydata(list(range(1900, 1951)), \"France\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking for those with birth month-days as January 1st\n",
    "\n",
    "For the reasons elaborated in sub-section 1.4 above, we need to make sure that those with birth month-days as January 1st have correct information (regarding their birth years). This section goes checks for these cases by going back to the original Wikidata *websites* (as opposed to the queries) to check for January 1st-cases.\n",
    "\n",
    "We also provide brief descriptions of the variables that are included in the final output:\n",
    "- `person`: Name of the person of interest\n",
    "- `personCode`: Wikidata URL of the said person\n",
    "- `birthplace`: Birth place of the said person, usually at the town or city level\n",
    "- `birthplaceCode`: Wikidata URL of `birthplace`\n",
    "- `birthregion`: Region to which `birthplace` belongs to\n",
    "- `birthregionCode`: Wikidata URL of `birthregion`\n",
    "- `birthgregion`: Region to which `birthregion` belongs to (i.e., \"greater\" region)\n",
    "- `birthgregionCode`: Wikidata URL of `birthgregion`\n",
    "- `birthggregion`: Region to which `birthgregion` belongs to (i.e., \"greater-greater\" region)\n",
    "- `birthggregionCode`: Wikidata URL of `birthggregion`\n",
    "- `occupation`: The said person's occupation\n",
    "- `occupationCode`: Wikidata URL of `occupation`\n",
    "- `dob_year`: Birth year of the person, returned by the Wikidata query\n",
    "- `dob_year_actual`: Birth year of the person, cross-checked with the Wikidata webpage\n",
    "- `dob_month`: Birth month of the person, returned by the Wikidata query\n",
    "- `dob_day`: Birth day of the person, returned by the Wikidata query\n",
    "- `sex`: Sex of the person\n",
    "\n",
    "Note that if a greater region to which `birthplace`, `birthregion`, or `birthgregion` belongs to does not exist (e.g., at the country level), greater region variables (e.g., `birthgregion` and `birthggregion` for `birthregion`) will be recorded as not available.\n",
    "\n",
    "\n",
    "### 2.1. Functions for checking the January 1st-birthdays and finalizing the cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jan1st_checker(wikidata_url, yr, verified):\n",
    "    \"\"\"\n",
    "    Checking, for a Wikidata entry, with the said entry's Wikidata website\n",
    "    to see if the year of birth is actually the specified year (i.e., `yr`)\n",
    "    from the Wikidata query\n",
    "\n",
    "    Inputs:\n",
    "    - wikidata_url (str): URL for the Wikidata entry\n",
    "    - yr (int): year of birth retrieved from the Wikidata query\n",
    "\n",
    "    Outputs:\n",
    "    - Either True (actual birth year is the year specified) or False (or not) in\n",
    "        non-error cases, or the str \"Error\" if there is some error while retrieving\n",
    "        information\n",
    "\n",
    "    \"\"\"\n",
    "    if type(verified) == bool:\n",
    "        return verified\n",
    "\n",
    "    ## following are the potential cases in which date of birth may appear\n",
    "    ## with VALID year of birth\n",
    "    monthday_filled = \"1 January {}\".format(yr)\n",
    "    monthday_filled2 = monthday_filled + \" Gregorian\"\n",
    "    monthday_filled3 = monthday_filled + \"Gregorian\"\n",
    "    simply_year = str(yr)\n",
    "    valid_cases = [monthday_filled, monthday_filled2, monthday_filled3, simply_year]\n",
    "\n",
    "    ## fetching the URL request and html information\n",
    "    url = wikidata_url.replace(\"/entity/\", \"/wiki/\")\n",
    "    url_req = req.get(url)\n",
    "    soup_data = bsoup(url_req.text, \"html.parser\")\n",
    "    first_dict = {\"data-property-id\": \"P569\"}  ## pertains to the data of birth info\n",
    "    second_dict = {\n",
    "        \"class\": \"wikibase-snakview-value wikibase-snakview-variation-valuesnak\"\n",
    "    }\n",
    "    try:\n",
    "        check_to = soup_data.find(attrs=first_dict).find_all(attrs=second_dict)\n",
    "        check_to_lst = [i.text for i in check_to]\n",
    "        intersection_check = np.intersect1d(valid_cases, check_to_lst)\n",
    "        if len(intersection_check) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except (AttributeError, ChunkedEncodingError) as error_tuple:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_merge_all_data(j1, nj1, cl=client):\n",
    "\n",
    "    j1_cases = j1[[\"personCode\", \"dob_year\"]].copy()\n",
    "    j1_cases[\"truth\"] = \"Error\"\n",
    "    j1_cases.drop_duplicates(inplace=True)\n",
    "    j1_cases.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    first = True\n",
    "    while j1_cases.dtypes[\"truth\"] == \"O\":\n",
    "        cl.restart()\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            error_n = j1_cases.loc[j1_cases.truth == \"Error\", :].shape[0]\n",
    "            print(\"Further detected {} unresolved cases, retrying..\".format(error_n))\n",
    "\n",
    "        wikidata_urls = list(j1_cases[\"personCode\"])\n",
    "        reported_years = list(j1_cases[\"dob_year\"])\n",
    "        retrieved_info = list(j1_cases[\"truth\"])\n",
    "        all_cases_j1 = tuple(zip(wikidata_urls, reported_years, retrieved_info))\n",
    "\n",
    "        check_fn = lambda x: jan1st_checker(x[0], x[1])\n",
    "        j1_mapped = cl.map(check_fn, all_cases_j1)\n",
    "        j1_cases[\"truth\"] = cl.gather(j1_mapped)\n",
    "\n",
    "    j1_cases[\"dob_year_actual\"] = j1_cases[\"dob_year\"].values\n",
    "    j1_cases.loc[~j1_cases.truth, \"dob_year_actual\"] = np.nan\n",
    "    j1_cases = j1_cases.drop([\"truth\"], axis=1).set_index([\"personCode\", \"dob_year\"])\n",
    "    j1_merged = j1.set_index([\"personCode\", \"dob_year\"])\n",
    "    j1_merged = j1_merged.merge(\n",
    "        j1_cases, left_index=True, right_index=True, how=\"left\"\n",
    "    ).reset_index()\n",
    "\n",
    "    total_cases = nj1.copy()\n",
    "    total_cases[\"dob_year_actual\"] = total_cases[\"dob_year\"].values\n",
    "    total_cases = pd.concat([total_cases, j1_merged], axis=0)\n",
    "    total_cases_columns = [\n",
    "        \"person\",\n",
    "        \"personCode\",\n",
    "        \"birthplace\",\n",
    "        \"birthplaceCode\",\n",
    "        \"birthregion\",\n",
    "        \"birthregionCode\",\n",
    "        \"birthgregion\",\n",
    "        \"birthgregionCode\",\n",
    "        \"birthggregion\",\n",
    "        \"birthggregionCode\",\n",
    "        \"occupation\",\n",
    "        \"occupationCode\",\n",
    "        \"dob_year\",\n",
    "        \"dob_year_actual\",\n",
    "        \"dob_month\",\n",
    "        \"dob_day\",\n",
    "        \"sex\",\n",
    "    ]\n",
    "    total_cases = (\n",
    "        total_cases[total_cases_columns]\n",
    "        .sort_values([\"dob_year\", \"dob_month\", \"dob_day\", \"person\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return total_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Demonstration for this section's codes\n",
    "\n",
    "We use `EXAMPLE_J1` and `EXAMPLE_NJ1` acquired from sub-section 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_MERGED_CASE = clean_and_merge_all_data(EXAMPLE_J1, EXAMPLE_NJ1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export (in `.csv` format, say), one can simply follow the below code. Note that `DESIRED_LOC` and `FILE_NAME` need to be changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_LOC = \"example_directory\"\n",
    "FILE_NAME = \"example_file_name.csv\"\n",
    "EXAMPLE_MERGED_CASE.to_csv(\"/\".join([DESIRED_LOC, FILE_NAME]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final remarks for this file\n",
    "\n",
    "In our application for **Peña and Choi (2021, *Economics Letters*)**, we ran a similar code with each of the 30 countries specified in `wiki_dict` above (also shown in the paper as well) and for the birth years 1700 to 2000 (inclusive).\n",
    "\n",
    "Also, it should be noted that the output from this file may have multiple rows for a same person of interest, due to reasons such as the said person's birthplace belonging to multiple districts or jurisdictions. These redundancies will be resolved in our Stata `.do` files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
